summary: create a recovery system and reboot into it

details: |
  This test creates a recovery system and validates that the newly created
  system can be rebooted into.

systems: [ubuntu-22.04-64, ubuntu-24.04-64]

environment:
  NESTED_CUSTOM_MODEL: $TESTSLIB/assertions/developer1-{VERSION}-dangerous.model
  NESTED_ENABLE_TPM: true
  NESTED_ENABLE_SECURE_BOOT: true
  NESTED_BUILD_SNAPD_FROM_CURRENT: true
  NESTED_REPACK_GADGET_SNAP: true
  NESTED_REPACK_KERNEL_SNAP: true
  NESTED_REPACK_BASE_SNAP: true
  NESTED_REPACK_FOR_FAKESTORE: true
  NESTED_FAKESTORE_BLOB_DIR: $(pwd)/fake-store-blobdir
  NESTED_SIGN_SNAPS_FAKESTORE: true
  NESTED_UBUNTU_IMAGE_SNAPPY_FORCE_SAS_URL: http://localhost:11028

  MODE/recover: "recover"
  MODE/factory_reset: "factory-reset"
  MODE/install: "install"

  # TODO: figure out a way to do this test without disabling secure boot and TMP
  # see tests/nested/core/core20-reinstall-partitions/task.yaml for more details
  NESTED_ENABLE_SECURE_BOOT/install: false
  NESTED_ENABLE_TPM/install: false

prepare: |
    if [ "${TRUST_TEST_KEYS}" = "false" ]; then
        echo "This test needs test keys to be trusted"
        exit
    fi

    # although nested_start_core_vm_unit usually installs this, the fake store
    # will already have been set up, so we need to install it here
    snap install test-snapd-swtpm --edge

    "${TESTSTOOLS}/store-state" setup-fake-store "${NESTED_FAKESTORE_BLOB_DIR}"

    cp "${TESTSLIB}/assertions/testrootorg-store.account-key" "${NESTED_FAKESTORE_BLOB_DIR}/asserts"
    cp "${TESTSLIB}/assertions/developer1.account" "${NESTED_FAKESTORE_BLOB_DIR}/asserts"
    cp "${TESTSLIB}/assertions/developer1.account-key" "${NESTED_FAKESTORE_BLOB_DIR}/asserts"

    snap ack "$NESTED_FAKESTORE_BLOB_DIR/asserts/testrootorg-store.account-key"
    snap ack "$NESTED_FAKESTORE_BLOB_DIR/asserts/developer1.account"
    snap ack "$NESTED_FAKESTORE_BLOB_DIR/asserts/developer1.account-key"

    version="$(tests.nested show version)"
    gendeveloper1 sign-model < "./vset-${version}.json" > validation-set.assert
    cp validation-set.assert "${NESTED_FAKESTORE_BLOB_DIR}/asserts"

    # run the fake device service too, so that the device can be initialised
    systemd-run --collect --unit fakedevicesvc fakedevicesvc localhost:11029

    tests.nested build-image core
    tests.nested create-vm core

    #shellcheck source=tests/lib/core-config.sh
    . "$TESTSLIB"/core-config.sh
    wait_for_first_boot_change

    # do we have a better way to do this?
    remote.exec 'sudo systemctl stop snapd.service snapd.socket'
    remote.exec 'sudo cat /var/lib/snapd/state.json' | gojq '.data.auth.device."session-macaroon"="fake-session"' > state.json
    remote.push state.json
    remote.exec 'sudo mv state.json /var/lib/snapd/state.json'
    remote.exec 'sudo systemctl start snapd.service snapd.socket'

    # before adding our new snaps to the fake store, disable auto-refreshes
    remote.refresh disable-refreshes

    declare -A snap_ids=(
      ["core22"]="amcUKQILKXHHTlmSa7NMdnXSx02dNeeT"
      ["core24"]="dwTAh7MZZ01zyriOZErqd1JynQLiOGvM"
      ["pc-kernel"]="pYVQrBcKmBa0mZ4CCN7ExT6jH8rY1hza"
      ["pc"]="UqFziVZDHLSyO3TqSWgNBoAdHbLI4dAH"
    )

    # make some new snaps available in the fakestore, we'll use these to test
    # creating the recovery system
    for snap in "core${version}" pc-kernel pc; do
      unsquashfs "${NESTED_FAKESTORE_BLOB_DIR}/${snap}.snap"
      gojq --yaml-input --yaml-output '.version = "v2"' squashfs-root/meta/snap.yaml > snap.yaml.new
      mv snap.yaml.new squashfs-root/meta/snap.yaml
      snap pack --filename="./${snap}.snap" ./squashfs-root
      rm -rf squashfs-root

      "${TESTSTOOLS}"/store-state make-snap-installable \
        --revision 2 \
        "${NESTED_FAKESTORE_BLOB_DIR}" \
        "./${snap}.snap" \
        "${snap_ids[${snap}]}"

      rm "./${snap}.snap"
    done

restore: |
    "$TESTSTOOLS"/store-state teardown-fake-store "$NESTED_FAKESTORE_BLOB_DIR"
    systemctl stop fakedevicesvc.service || true

execute: |
  function post_json_data() {
    route=$1
    template=$2
    shift 2

    # shellcheck disable=SC2059
    response=$(printf "${template}" "$@" | remote.exec "sudo snap debug api -X POST -H 'Content-Type: application/json' ${route}")
    if ! gojq -e .change <<< "${response}"; then
      echo "could not get change id from response: ${response}"
      false
    fi
  }

  boot_id="$(tests.nested boot-id)"
  prev_system=$(remote.exec 'sudo snap recovery' | awk 'NR != 1 { print $1 }')
  version="$(tests.nested show version)"

  # create the system
  change_id=$(post_json_data /v2/systems '{"action": "create", "label": "new-system", "validation-sets": ["developer1/pinned-essential-snaps-%s"], "mark-default": true, "test-system": true}' "${version}")

  # wait for reboot since we tested the system
  remote.wait-for reboot "${boot_id}"
  boot_id="$(tests.nested boot-id)"

  # configure the proxy in the recovered vm if required
  tests.nested setup-vm

  remote.wait-for snap-command

  remote.exec snap watch "${change_id}"

  remote.exec 'test -d /run/mnt/ubuntu-seed/systems/new-system'
  remote.exec 'sudo cat /var/lib/snapd/modeenv' > modeenv
  MATCH 'current_recovery_systems=.*,new-system$' < modeenv
  MATCH 'good_recovery_systems=.*,new-system$' < modeenv

  remote.exec 'sudo snap recovery' | awk '$1 == "new-system" { print $4 }' | MATCH 'default-recovery'

  remote.exec "sudo snap reboot --${MODE}" || true
  remote.wait-for reboot "${boot_id}"

  # configure the proxy in the recovered vm if required
  tests.nested setup-vm

  remote.wait-for snap-command

  # wait for the system to finish being seeded
  remote.exec "sudo snap wait system seed.loaded"

  boot_id="$(tests.nested boot-id)"

  if [ "${MODE}" = 'recover' ]; then
    remote.exec 'cat /proc/cmdline' | MATCH 'snapd_recovery_mode=recover'
    remote.exec 'sudo cat /var/lib/snapd/modeenv' > modeenv
    MATCH 'mode=recover' < modeenv
    MATCH 'recovery_system=new-system' < modeenv
  elif [ "${MODE}" = 'factory-reset' ] || [ "${MODE}" = "install" ]; then
    if [ "${MODE}" = 'install' ]; then
      remote.wait-for device-initialized
    fi

    # should be back into run mode since we reset the device
    remote.exec cat /proc/cmdline | MATCH 'snapd_recovery_mode=run'

    # new system should be the default recovery system and the current system
    remote.exec 'sudo snap recovery' | awk '$1 == "new-system" { print $4 }' | MATCH 'current,default-recovery'

    # since out new system is now the default and the current recovery system,
    # we should be able to remove the old one
    post_json_data "/v2/systems/${prev_system}" '{"action": "remove"}'

    remote.exec "snap watch --last=remove-recovery-system"
    remote.exec "sudo snap recovery" | NOMATCH "${prev_system}"
  fi

  # make sure that all our other snaps are there too
  remote.exec "snap list core${version}"
  remote.exec 'snap list pc'
  remote.exec 'snap list pc-kernel'

  remote.exec "test -f /var/lib/snapd/seed/snaps/core${version}_2.snap"
  remote.exec "test -f /var/lib/snapd/seed/snaps/pc-kernel_2.snap"
  remote.exec "test -f /var/lib/snapd/seed/snaps/pc_2.snap"

summary: |
    Test that cloud-init is no longer vulnerable on Ubuntu Core with the fix for
    CVE-2020-11933 in place.

details: |
    CVE-2020-11933 allowed anyone to present the device with cloud-info
    meta-data, for example on a removable media, and reboot the machine to gain
    elevated privileges. The test ensures that once snapd snap is refreshed to a
    version which contains the fix, cloud-init gets disabled.

systems: [ubuntu-18.04-64, ubuntu-16.04-64]

environment:
    # this test ensures that existing images without the fix are no longer
    # vulnerable after refreshing to a version of snapd with the fix
    NESTED_BUILD_SNAPD_FROM_CURRENT/refresh: false
    NESTED_BUILD_SNAPD_FROM_CURRENT/firstboot: true
    NESTED_USE_CLOUD_INIT: false

    # this test is only running on nested systems, so only amd64 for now
    SNAPD_2_45_SNAPD_SNAP: https://storage.googleapis.com/snapd-spread-tests/snaps/snapd_2.45_7777.snap
    SNAPD_2_45_CORE_SNAP: https://storage.googleapis.com/snapd-spread-tests/snaps/core_2.45_9289.snap

prepare: |
    # build an unrelated empty cdrom drive to provide to first boot with no 
    # real files on it to use as a placeholder in qemu args
    tests.nested build-seed "$TESTSLIB/cloud-init-seeds/emptykthxbai" seed.iso notcidata emptykthxbai

    # build the attacker cloud-init NoCloud cdrom drive
    tests.nested build-seed "$TESTSLIB/cloud-init-seeds/attacker-user" seed2.iso cidata user-data meta-data

    # if we are not building from current, then we need to seed the specific,
    # old version of snapd that was vulnerable into the image to start there,
    # otherwise we will start from stable or edge which already has the fix and
    # thus this test would no longer be testing that old vulnerable devices 
    # become safe after refreshing to the fix
    if [ "$NESTED_BUILD_SNAPD_FROM_CURRENT" = "false" ]; then
        snap install test-snapd-curl --edge --devmode # devmode so it can save to any dir

        if os.query is-xenial; then
            # uc16 uses core snap
            test-snapd-curl.curl -s -o core_2.45.snap "$SNAPD_2_45_CORE_SNAP"
            # The core snap is unpacked and repacked to prevent it is auto-refreshed when
            # it is installed with --dangerous as it has a different hash
            unsquashfs -d core-snap core_2.45.snap
            snap pack core-snap/ "$(tests.nested get extra-snaps-path)"
        else 
            # uc18 uses snapd snap
            test-snapd-curl.curl -s -o snapd_2.45.snap "$SNAPD_2_45_SNAPD_SNAP"
            # The snapd snap is unpacked and repacked to prevent it from being
            # auto-refreshed when it is installed
            unsquashfs -d snapd-snap snapd_2.45.snap
            snap pack snapd-snap/ "$(tests.nested get extra-snaps-path)"
        fi
    fi

    tests.nested build-image core 

    # first boot will use seed1 which is empty, but the same file name will be 
    # replace while the VM is shutdown to use the second attacker iso
    tests.nested create-vm core --param-cdrom "-cdrom $(pwd)/seed.iso"

debug: |
    echo "logs before reboot"
    cat snapd-before-reboot.logs || true

    echo "logs from current nested VM boot snapd"
    remote.exec "sudo journalctl -e --no-pager -u snapd" || true

execute: |
    echo "The VM here will not ever had used cloud-init so snapd should disable"
    # cloud-init when it is installed

    echo "Wait for done seeding"
    remote.wait-for snap-command
    remote.exec "sudo snap wait system seed.loaded"

    echo "Prepare snapd snap to install with the fix"
    # if we are not building from current, then we need to prep the snapd snap
    # to install with the fix, this simulates/verifies that devices in the field
    # without the fix will actually be fixed after they refresh
    if [ "$NESTED_BUILD_SNAPD_FROM_CURRENT" = "false" ]; then
        if os.query is-xenial; then
            # build the core snap for this run
            "$TESTSTOOLS"/snaps-state repack_snapd_deb_into_snap core "$PWD"
            remote.push "$PWD/core-from-snapd-deb.snap"

            # install the core snap
            remote.exec "sudo snap install core-from-snapd-deb.snap --dangerous"

            # now we wait for the reboot for the new core snap
            tests.nested wait-for no-ssh
            tests.nested wait-for ssh
            
        else
            # build the snapd snap for this run
            "$TESTSTOOLS"/snaps-state repack_snapd_deb_into_snap snapd
            remote.push "$PWD/snapd-from-deb.snap"

            # install the snapd snap
            remote.exec "sudo snap install snapd-from-deb.snap --dangerous"
        fi
    fi

    # Note: there is a race here after we have installed the fix (or we have 
    # booted a fresh image with the fix). 
    # Namely, snapd will begin checking on cloud-init status after it has 
    # ensured that the boot was okay in the device manager, but this will not
    # happen immediately in zero time, and moreover, snapd will not do anything
    # substantial or measurable until cloud-init has reached a steady state or
    # otherwise times out. 
    # As such, in this test, we first wait for cloud-init to settle down, and 
    # then wait a bit longer to give snapd a chance to run again and take 
    # action that we can test for. Since in this test, cloud-init was never 
    # used, the status command should not take much time at all, since it will 
    # not have been triggered. 

    echo "Waiting for cloud-init..."
    remote.exec "cloud-init status --wait"

    echo "Waiting for snapd to react to cloud-init"
    # It is needed || true because in ubuntu-core-16 there is not any cloud-init message in the journal log
    retry --wait 1 -n 60 sh -c 'remote.exec sudo journalctl --no-pager -u snapd | MATCH "cloud-init reported"'

    # ensure that snapd disabled cloud-init with the cloud-init.disabled file
    echo "Ensuring that snapd restricted cloud-init"
    remote.exec "cloud-init status" | MATCH "status: disabled"
    remote.exec "test -f /etc/cloud/cloud-init.disabled"
    remote.exec "! test -f /etc/cloud/cloud.cfg.d/zzzz_snapd.cfg"

    echo "Save snapd logs before continuing as the logs are not persistent"
    remote.exec "sudo journalctl -e --no-pager -u snapd" > snapd-before-reboot.logs

    echo "Gracefully shutting down the nested VM to prepare a simulated attack"
    boot_id="$(tests.nested boot-id)"
    tests.nested vm stop

    echo "Replace the empty seed.iso with the new attacker iso"
    mv seed2.iso seed.iso

    echo "Restarting nested VM with attacker cloud-init CD-ROM drive"
    tests.nested vm start
    remote.wait-for reboot "${boot_id}"

    # cloud-init should not actually run, since it was disabled, but in case the
    # test fails, for a better error, we will wait for cloud-init to report that
    # it is "done" or at least steady before ensuring that the attacker-user was
    # not created.
    echo "Waiting for cloud-init..."
    remote.exec "cloud-init status --wait"

    # the attacker-user should not have been created
    echo "The cloud-init user was not created"
    remote.exec "cat /var/lib/extrausers/passwd" | NOMATCH attacker-user

    # cloud-init should still be disabled
    echo "cloud-init is still disabled"
    remote.exec "cloud-init status" | MATCH "status: disabled"
    remote.exec "test -f /etc/cloud/cloud-init.disabled"
    remote.exec "! test -f /etc/cloud/cloud.cfg.d/zzzz_snapd.cfg"
